---
title: "When the Timeline Meets the Pipeline: A Survey on Automated Cyberbullying Detection"
collection: publications
type: "Journal Paper"
date: 2021-07-21
authors: "Fatma Elsafoury, Stamos Katsigiannis, Zeeshan Pervez, and Naeem Ramzan"
proceedings: "IEEE Access (2021)"
venue: "IEEE Access"
#paperurl: 'https://ieeexplore.ieee.org/document/9492047?source=authoralert'
bibtexurl: '/files/publications/2021/ieee_access.bib'
---
<a href="/files/publications/2021/When_the_Timeline_Meets_the_Pipeline_A_Survey_on_Automated_Cyberbullying_Detection.pdf"><img src="/images/paper_symbol.png" alt="Link to paper" style="width:42px;height:42px;"></a>


**Abstract:** Web 2.0 helped user-generated platforms to spread widely. Unfortunately, it also allowed for cyberbullying to spread. Cyberbullying has negative effects that could lead to cases of depression and low self-esteem. It has become crucial to develop tools for automated cyberbullying detection. The research on developing these tools has been growing over the last decade, especially with the recent advances in machine learning and natural language processing. Given the large body of work on this topic, it is vital to critically review the literature on cyberbullying within the context of these latest advances. In this paper, we survey the automated detection of cyberbullying. Our survey sheds light on some challenges and limitations for the field. The challenges range from defining cyberbullying, data collection, and feature representation to model selection, training, and evaluation. We also provide some suggestions for improving the task of cyberbullying detection. In addition to the survey, we propose to improve the task of cyberbullying detection by addressing some of the raised limitations: 1) Using recent contextual language models like BERT for the detection of cyberbullying; 2) Using slang-based word embeddings to generate better representations of the cyberbullying-related datasets. Our results show that BERT outperforms state-of-the-art cyberbullying detection models and deep learning models. The results also show that deep learning models initialized with slang-based word embeddings outperform deep learning models initialized with traditional word embeddings.