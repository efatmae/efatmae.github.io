---
title: "Does BERT Pay Attention To Attribution?"
collection: "Invited talks"
type: "Invited talks"
permalink: /talks/2020-11-EdLunch
venue: "72 Language at Edinburgh Lunch"
date: 2020-11-1
location: "Online"
---
Abstract:
==========
Language models like the Bidirectional Encoder Representations from Transformers (BERT) have been established as the new state of the art in Natural Language Processing (NLP). These models use attention mechanisms, which despite their good performance, it is unclear how they affect the model's outcome. There have been some studies that either prove that  not all attention heads are contributing much to the models performance or that there is no correlation between attention weights and the model's outcome. On the other hand, some studies claim that attention provides some form of explanation to the model's performance. All these studies have either used attention mechanisms with deep neural networks other than BERT, or analysed the attention weights of BERT on tasks other than text classification, mostly using sentiment analysis datasets like IMDB or SST2. In this paper, we examined the pattern of attention weights of fine-tuned BERT on text classification tasks on multiple cyberbullying-related datasets and provided an analysis of their relationship with its outcome. Our findings show that although fine-tuning does change the pattern of BERT's attention weights, there is no evidence that the attention weights of fine-tuned BERT play a direct role in its outcome.

<a href="/files/talks/2020/2020-11-EdLunch.pdf">Talk presentation</a>
